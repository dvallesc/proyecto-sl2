{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diego Fernando Valle Morales\n",
    "### 20003022\n",
    "\n",
    "# Parte 3 - Recurrent Neural Network\n",
    "\n",
    "#### Acerca del dataset\n",
    "En esta tercera parte se ha procedido a realizar una Red Neural Recurrente o RNN usando un dataset de canciones, el cual ha sido obtenido en Github y contiene 57,650 canciones de diferentes artistas, el dataset contiene canciones de 643 artistas de diferentes géneros musicales y de diferentes épocas.\n",
    "\n",
    "#### Objetivo de la RNN\n",
    "Generar letras de canciones fusionando letras de dos artistas que están separados en el tiempo, pero que sería interesante ver que sucedería, se utilizarán \"Bon Jovi\" y \"The Beatles\".\n",
    "\n",
    "Se utilizo para este problema una red convolucional con LSTM. A continuación se describe que es una red LSTM.\n",
    "\n",
    "## ENSAYO: Arquitectura Long Short Term Memory (LSTM)\n",
    "\n",
    "La LSTM y su celda de memoria de corto plazo tiene ventajas sobre las celdas SimpleRNN y GRU (Gated Recurrent Unit) al poder retener aún más información más adelante en la secuencia de lared.\n",
    "\n",
    "La LSTM utiliza tres puertas diferentes en oposición a las dos que tiene GRU y retiene un estado de celda en toda la red. Se sabe que GRU tiene la ventaja de la velocidad sobre LSTM, ya que puede generalizar más rápido y usar menos parámetros.\n",
    "\n",
    "Sin embargo,  el LSTM tiende a ser mejor cuando se trata de retener más datos contextuales a lo largo de una secuencia.\n",
    "\n",
    "La celda LSTM puede ser expresada matemáticamente como:\n",
    "\n",
    "\n",
    "$ f_{(t)}=\\sigma (W_{xf}x_{(t)}+W_{hf}x_{(t-1)}+b_{f}) $\n",
    "\n",
    "$ i_{(t)}=\\sigma (W_{xi}x_{(t)}+W_{hi}x_{(t-1)}+b_{i}) $\n",
    "\n",
    "$ o_{(t)}=\\sigma (W_{xo}x_{(t)}+W_{ho}x_{(t-1)}+b_{o}) $\n",
    "\n",
    "$ c_{(t)}=f_{t}*c_{(t-1)}+i_{(t)}*tanh(W_{xc}x_{(t)}+W_{hc}h_{(t-1)}+b_{c}) $\n",
    "\n",
    "$ h_{(t)}=o_({t})* tanh(c_{(t)}) $\n",
    "\n",
    "Donde $ f(t) $ representa la puerta de olvido y determina que tanto debe olvidar del estado anterior.\n",
    "\n",
    "Luego $ i(t) $ representa las puertas de entrada que determinan que tanta de la nueva información se agregará a la celda de estado. \n",
    "\n",
    "La función $ o(t) $ es lla puerta de salida, que determina que información estará progresando al siguiente estado oculto. \n",
    "\n",
    "El estado de la celda esta representado por $ c(t) $, y el estado oculto es $ h(t) $.\n",
    "\n",
    "A continuación un gráfico de cómo funciona:\n",
    "\n",
    "![SegmentLocal](CeldaLSTM.gif \"segment\")\n",
    "\n",
    "Paper en el que se inspiró este proyecto se llama **Evaluating Creative Language Generation: The Case of Rap Lyric Ghostwriting** Elaborado por _Peter Potash, Alexey Romanov, Anna Rumshisky_ de la University of Massachusetts Lowell - Departament of Computer Science en Diciembre del 2016. Link: \n",
    "https://arxiv.org/pdf/1612.03205.pdf \n",
    "\n",
    "Básicamente en este paper se mencional que las tareas de generación del lenguaje que buscan imitar la habilidad humana para usar un lenguaje creativamente son difíciles de evaluar, desde que uno debe considerar la creatividad, el estilo y otros aspectos no triviales del texto que es generado. La meta que define el paper es desarrollar metodos de evaluación para una tarea, \"ghostwriting\" de letras de rap, y proveer de forma explicita, bases cuantificables para las metas y futoras directrices para esta tarea.\n",
    "\n",
    "\"Ghostwriting\" debe producir texto que es similar en estilo al artista emulado, aunque distinto en contenido. Se desarrolla en el documento una nueva metodología de evaluación que dirige muchos aspectos complementarios de esta tarea e ilustra como tal evaluación puede ser usada para analizar el rendimiento del sistema de forma significativa.\n",
    "\n",
    "El paper provee un corpus de canciones de trece artistas de rap, que destacan por su similaridad en el estilo que permite alcanzar la factibilidad de un verso generado de forma manual.\n",
    "\n",
    "Se puede decir que este paper tiene como objetivo indirecto resultar como un punto de partida para futuros modelos generativos, tomando en consideración que LSTM no tiene la habilidad de integrar nuevos vocabularios, entonces estos futuros modelos generativos pueden dibucar sobre el trabajo de Graves en el año 2013 y de Bowman entre otros en el 2015 en un intento por hacer un apalancamiento entre otras letras de artistas.\n",
    "\n",
    "A continuación se presenta un listado de varias referencias que pueden servir como complementarias a este paper:\n",
    "\n",
    "* Gabriele Barbieri, Franc¸ois Pachet, Pierre Roy, and Mirko Degli Esposti. 2012. Markov constraints for generating lyrics with style. In ECAI, pages 115–120.\n",
    "* Anja Belz and Eric Kow. 2011. Discrete vs. continuous rating scales for language evaluation in nlp. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 230–235. Association for Computational Linguistics. \n",
    "* Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media.\n",
    "* Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio.\n",
    "2015. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349.\n",
    "* Leon A Gatys, Alexander S Ecker, and Matthias Bethge. 2015. A neural algorithm of artistic style. arXiv preprint arXiv:1508.06576.\n",
    "* Pablo Gervas. 2000. Wasp: Evaluation of different strategies for the automatic generation of spanish verse. In Proceedings of the AISB-00 Symposium on Creative & Cultural Aspects of AI, pages 93–100.\n",
    "* Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.\n",
    "* Helen Hastie and Anja Belz. 2014. A comparative evaluation methodology for nlg in interactive systems.\n",
    "* Hussein Hirjee and Daniel Brown. 2010a. Using automated rhyme detection to characterize rhyming style in rap music.\n",
    "* Hussein Hirjee and Daniel G Brown. 2010b. Rhyme analyzer: An analysis tool for rap lyrics. In Proceedings of the 11th International Society for Music Information Retrieval Conference. Citeseer.\n",
    "* Chuan Li and Michael Wand. 2016. Combining markov random fields and convolutional neural networks for image synthesis. arXiv preprint arXiv:1601.04589.\n",
    "* Eric Malmi, Pyry Takala, Hannu Toivonen, Tapani Raiko, and Aristides Gionis. 2015. Dopelearning: A computational approach to rap lyrics generation. arXiv preprint arXiv:1505.04771.\n",
    "* Hugo Goncalo Oliveira, Raquel Hervas, Alberto Díaz, and Pablo Gervas. 2014. Adapting a generic platform for poetry generation to produce spanish poems. In 5th International Conference on Computational Creativity, ICCC.\n",
    "* Peter Potash, Alexey Romanov, and Anna Rumshisky. 2015. Ghostwriter: Using an LSTM for automatic rap lyric generation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.\n",
    "* Dekai Wu, Karteek Addanki, Markus Saers, and Meriem Beloucif. 2013. Learning to freestyle: Hip hop challenge-response induction via transduction rule segmentation. In 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013), Seattle, Washington, USA.\n",
    "* Karteek Addanki Dekai Wu. 2014. Evaluating improvised hip hop lyrics–challenges and observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando librerías relacionadas con Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando otras librerías relacionadas con el proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se importa el dataset de canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>57650</td>\n",
       "      <td>57650</td>\n",
       "      <td>57650</td>\n",
       "      <td>57650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>643</td>\n",
       "      <td>44824</td>\n",
       "      <td>57650</td>\n",
       "      <td>57494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>Donna Summer</td>\n",
       "      <td>Have Yourself A Merry Little Christmas</td>\n",
       "      <td>/r/rod+stewart/can+we+still+be+friends_2011720...</td>\n",
       "      <td>I've got sunshine on a cloudy day  \\nWhen it's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>191</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              artist                                    song  \\\n",
       "count          57650                                   57650   \n",
       "unique           643                                   44824   \n",
       "top     Donna Summer  Have Yourself A Merry Little Christmas   \n",
       "freq             191                                      35   \n",
       "\n",
       "                                                     link  \\\n",
       "count                                               57650   \n",
       "unique                                              57650   \n",
       "top     /r/rod+stewart/can+we+still+be+friends_2011720...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                     text  \n",
       "count                                               57650  \n",
       "unique                                              57494  \n",
       "top     I've got sunshine on a cloudy day  \\nWhen it's...  \n",
       "freq                                                    6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# device = 'cpu'\n",
    "\n",
    "data = pd.read_csv('dataset/songdata.csv')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestra información del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at her face, it's a wonderful face  \\nAnd it means something special to me  \\nLook at the way that she smiles when she sees me  \\nHow lucky can one fellow be?  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, without her I'm blue  \\nAnd if she ever leaves me what could I do, what could I do?  \\n  \\nAnd when we go for a walk in the park  \\nAnd she holds me and squeezes my hand  \\nWe'll go on walking for hours and talking  \\nAbout all the things that we plan  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, without her I'm blue  \\nAnd if she ever leaves me what could I do, what could I do?\\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ABBA', 'Ace Of Base', 'Adam Sandler', 'Adele', 'Aerosmith',\n",
       "       'Air Supply', 'Aiza Seguerra', 'Alabama', 'Alan Parsons Project',\n",
       "       'Aled Jones', 'Alice Cooper', 'Alice In Chains', 'Alison Krauss',\n",
       "       'Allman Brothers Band', 'Alphaville', 'America', 'Amy Grant',\n",
       "       'Andrea Bocelli', 'Andy Williams', 'Annie', 'Ariana Grande',\n",
       "       'Ariel Rivera', 'Arlo Guthrie', 'Arrogant Worms', 'Avril Lavigne',\n",
       "       'Backstreet Boys', 'Barbie', 'Barbra Streisand', 'Beach Boys',\n",
       "       'The Beatles', 'Beautiful South', 'Beauty And The Beast',\n",
       "       'Bee Gees', 'Bette Midler', 'Bill Withers', 'Billie Holiday',\n",
       "       'Billy Joel', 'Bing Crosby', 'Black Sabbath', 'Blur', 'Bob Dylan',\n",
       "       'Bob Marley', 'Bob Rivers', 'Bob Seger', 'Bon Jovi', 'Boney M.',\n",
       "       'Bonnie Raitt', 'Bosson', 'Bread', 'Britney Spears',\n",
       "       'Bruce Springsteen', 'Bruno Mars', 'Bryan White', 'Cake',\n",
       "       'Carly Simon', 'Carol Banawa', 'Carpenters', 'Cat Stevens',\n",
       "       'Celine Dion', 'Chaka Khan', 'Cheap Trick', 'Cher', 'Chicago',\n",
       "       'Children', 'Chris Brown', 'Chris Rea', 'Christina Aguilera',\n",
       "       'Christina Perri', 'Christmas Songs', 'Christy Moore',\n",
       "       'Chuck Berry', 'Cinderella', 'Clash', 'Cliff Richard', 'Coldplay',\n",
       "       'Cole Porter', 'Conway Twitty', 'Counting Crows',\n",
       "       'Creedence Clearwater Revival', 'Crowded House', 'Culture Club',\n",
       "       'Cyndi Lauper', 'Dan Fogelberg', 'Dave Matthews Band',\n",
       "       'David Allan Coe', 'David Bowie', 'David Guetta', 'David Pomeranz',\n",
       "       'Dean Martin', 'Death', 'Deep Purple', 'Def Leppard',\n",
       "       'Demi Lovato', 'Depeche Mode', 'Devo', 'Dewa 19', 'Diana Ross',\n",
       "       'Dire Straits', 'Divine', 'Dolly Parton', 'Don Henley',\n",
       "       'Don McLean', 'Don Moen', 'Donna Summer', 'Doobie Brothers',\n",
       "       'Doors', 'Doris Day', 'Drake', 'Dream Theater',\n",
       "       'Dusty Springfield', 'Eagles', 'Ed Sheeran', 'Eddie Cochran',\n",
       "       'Electric Light Orchestra', 'Ella Fitzgerald', 'Ellie Goulding',\n",
       "       'Elton John', 'Elvis Costello', 'Elvis Presley', 'Eminem',\n",
       "       'Emmylou Harris', 'Engelbert Humperdinck', 'Enigma',\n",
       "       'Enrique Iglesias', 'Enya', 'Eppu Normaali', 'Erasure',\n",
       "       'Eric Clapton', 'Erik Santos', 'Etta James', 'Europe',\n",
       "       'Eurythmics', 'Evanescence', 'Everclear', 'Everlast', 'Exo',\n",
       "       'Exo-K', 'Extreme', 'Fabolous', 'Face To Face', 'Faces',\n",
       "       'Faith Hill', 'Faith No More', 'Falco', 'Fall Out Boy', 'Fastball',\n",
       "       'Fatboy Slim', 'Fifth Harmony', 'Fiona Apple', 'Fleetwood Mac',\n",
       "       'Flo-Rida', 'Foo Fighters', 'Foreigner', 'Frank Sinatra',\n",
       "       'Frank Zappa', 'Frankie Goes To Hollywood', 'Frankie Laine',\n",
       "       'Frankie Valli', 'Freddie Aguilar', 'Freddie King', 'Free',\n",
       "       'Freestyle', 'Fun.', 'Garth Brooks', 'Gary Numan',\n",
       "       'Gary Valenciano', 'Genesis', 'George Formby', 'George Harrison',\n",
       "       'George Jones', 'George Michael', 'George Strait', 'Gino Vannelli',\n",
       "       'Gipsy Kings', 'Glee', 'Glen Campbell', 'Gloria Estefan',\n",
       "       'Gloria Gaynor', 'GMB', 'Gordon Lightfoot', 'Grand Funk Railroad',\n",
       "       'Grateful Dead', 'Grease', 'Great Big Sea', 'Green Day',\n",
       "       'Gucci Mane', 'Guided By Voices', \"Guns N' Roses\", 'Halloween',\n",
       "       'Hank Snow', 'Hank Williams', 'Hank Williams Jr.', 'Hanson',\n",
       "       'Happy Mondays', 'Harry Belafonte', 'Harry Connick, Jr.', 'Heart',\n",
       "       'Helloween', 'High School Musical', 'Hillsong', 'Hillsong United',\n",
       "       'HIM', 'Hollies', 'Hooverphonic', 'Horrible Histories',\n",
       "       'Housemartins', 'Howard Jones', 'Human League', 'Ian Hunter',\n",
       "       'Ice Cube', 'Idina Menzel', 'Iggy Pop', 'Il Divo',\n",
       "       'Imagine Dragons', 'Imago', 'Imperials', 'Incognito', 'Incubus',\n",
       "       'Independence Day', 'Indiana Bible College', 'Indigo Girls',\n",
       "       'Ingrid Michaelson', 'Inna', 'Insane Clown Posse', 'Inside Out',\n",
       "       'INXS', 'Iron Butterfly', 'Iron Maiden', 'Irving Berlin',\n",
       "       'Isley Brothers', 'Israel', 'Israel Houghton', 'Iwan Fals',\n",
       "       'J Cole', 'Jackson Browne', 'The Jam', 'James Taylor',\n",
       "       'Janis Joplin', 'Jason Mraz', 'Jennifer Lopez', 'Jim Croce',\n",
       "       'Jimi Hendrix', 'Jimmy Buffett', 'John Denver', 'John Legend',\n",
       "       'John Martyn', 'John McDermott', 'John Mellencamp', 'John Prine',\n",
       "       'John Waite', 'Johnny Cash', 'Joni Mitchell', 'Jose Mari Chan',\n",
       "       'Josh Groban', 'Journey', 'Joy Division', 'Judas Priest', 'Judds',\n",
       "       'Judy Garland', 'Justin Bieber', 'Justin Timberlake', 'Kanye West',\n",
       "       'Kari Jobe', 'Kate Bush', 'Katy Perry', 'Keith Green',\n",
       "       'Keith Urban', 'Kelly Clarkson', 'Kelly Family', 'Kenny Chesney',\n",
       "       'Kenny Loggins', 'Kenny Rogers', 'Kid Rock', 'The Killers',\n",
       "       'Kim Wilde', 'King Crimson', 'King Diamond', 'Kinks',\n",
       "       'Kirk Franklin', 'Kirsty Maccoll', 'Kiss', 'Koes Plus', 'Korn',\n",
       "       'Kris Kristofferson', 'Kyla', 'Kylie Minogue', 'Lady Gaga',\n",
       "       'Lana Del Rey', 'Lata Mangeshkar', 'Lauryn Hill', 'Lea Salonga',\n",
       "       'Leann Rimes', 'Lenny Kravitz', 'Leo Sayer', 'Leonard Cohen',\n",
       "       'Les Miserables', 'Lil Wayne', 'Linda Ronstadt', 'Linkin Park',\n",
       "       'Lionel Richie', 'Little Mix', 'Little Walter', 'LL Cool J',\n",
       "       'Lloyd Cole', 'Lorde', 'Loretta Lynn', 'Lou Reed',\n",
       "       'Louis Armstrong', 'Louis Jordan', 'Lucky Dube', 'Luther Vandross',\n",
       "       'Lynyrd Skynyrd', 'Madonna', 'Manowar', 'Mariah Carey',\n",
       "       'Marianne Faithfull', 'Marillion', 'Marilyn Manson', 'Mark Ronson',\n",
       "       'Maroon 5', 'Mary Black', 'Matt Monro', 'Matt Redman',\n",
       "       'Mazzy Star', 'Mc Hammer', 'Meat Loaf', 'Megadeth', 'Men At Work',\n",
       "       'Metallica', 'Michael Bolton', 'Michael Buble', 'Michael Jackson',\n",
       "       'Michael W. Smith', 'Migos', 'Miley Cyrus', 'Misfits',\n",
       "       'Modern Talking', 'The Monkees', 'Moody Blues', 'Morrissey', 'Mud',\n",
       "       \"'n Sync\", 'Nat King Cole', 'Natalie Cole', 'Natalie Grant',\n",
       "       'Natalie Imbruglia', 'Nazareth', 'Ne-Yo', 'Neil Diamond',\n",
       "       'Neil Sedaka', 'Neil Young', 'New Order', 'Next To Normal',\n",
       "       'Nick Cave', 'Nick Drake', 'Nickelback', 'Nicki Minaj',\n",
       "       'Nightwish', 'Nina Simone', 'Nine Inch Nails', 'Nirvana',\n",
       "       'Nitty Gritty Dirt Band', 'Noa', 'NOFX', 'Norah Jones',\n",
       "       'Notorious B.I.G.', 'O-Zone', 'O.A.R.', 'Oasis',\n",
       "       'Ocean Colour Scene', 'Offspring', 'Ofra Haza', 'Oingo Boingo',\n",
       "       \"Old 97's\", 'Oliver', 'Olivia Newton-John', 'Olly Murs', 'Omd',\n",
       "       'One Direction', 'OneRepublic', 'Opeth', 'Orphaned Land',\n",
       "       'Oscar Hammerstein', 'Otis Redding', 'Our Lady Peace',\n",
       "       'Out Of Eden', 'Outkast', 'Overkill', 'Owl City', 'Ozzy Osbourne',\n",
       "       'Passenger', 'Pat Benatar', 'Patsy Cline', 'Patti Smith',\n",
       "       'Paul McCartney', 'Paul Simon', 'Pearl Jam', 'Perry Como',\n",
       "       'Pet Shop Boys', 'Peter Cetera', 'Peter Gabriel', 'Peter Tosh',\n",
       "       'Pharrell Williams', 'Phil Collins', 'Phineas And Ferb', 'Phish',\n",
       "       'Pink Floyd', 'Pitbull', 'Planetshakers', 'P!nk', 'Pogues',\n",
       "       'Point Of Grace', 'Poison', 'Pretenders', 'Primus', 'Prince',\n",
       "       'Proclaimers', 'Procol Harum', 'Puff Daddy', 'Q-Tip', 'Qntal',\n",
       "       'Quarashi', 'Quarterflash', 'Quasi', 'Queen', 'Queen Adreena',\n",
       "       'Queen Latifah', 'Queens Of The Stone Age', 'Queensryche',\n",
       "       'Quicksand', 'Quicksilver Messenger Service', 'Quiet Riot',\n",
       "       'Quietdrive', 'Quincy Jones', 'Quincy Punx', 'R. Kelly',\n",
       "       'Radiohead', 'Raffi', 'Rage Against The Machine', 'Rainbow',\n",
       "       'Rammstein', 'Ramones', 'Randy Travis', 'Rascal Flatts',\n",
       "       'Ray Boltz', 'Ray Charles', 'Reba Mcentire',\n",
       "       'Red Hot Chili Peppers', 'Regine Velasquez', 'Religious Music',\n",
       "       'Rem', 'Reo Speedwagon', 'Richard Marx', 'Rick Astley', 'Rihanna',\n",
       "       'Robbie Williams', 'Rod Stewart', 'Rolling Stones', 'Roxette',\n",
       "       'Roxy Music', 'Roy Orbison', 'Rush', 'Sam Smith', 'Santana',\n",
       "       'Savage Garden', 'Scorpions', 'Selah', 'Selena Gomez', 'Sia',\n",
       "       'Side A', 'Slayer', 'Smiths', 'Snoop Dogg', 'Soundgarden',\n",
       "       'Spandau Ballet', 'Squeeze', 'Starship', 'Status Quo',\n",
       "       'Steely Dan', 'Steve Miller Band', 'Stevie Ray Vaughan',\n",
       "       'Stevie Wonder', 'Sting', 'Stone Roses', 'Stone Temple Pilots',\n",
       "       'Styx', 'Sublime', 'Supertramp', 'System Of A Down',\n",
       "       'Talking Heads', 'Taylor Swift', 'Tears For Fears',\n",
       "       'The Temptations', 'Ten Years After', 'The Broadways',\n",
       "       'The Script', 'The Weeknd', 'Thin Lizzy', 'Tiffany', 'Tim Buckley',\n",
       "       'Tim McGraw', 'Tina Turner', 'Tom Jones', 'Tom Lehrer',\n",
       "       'Tom T. Hall', 'Tom Waits', 'Tool', 'Tori Amos', 'Toto',\n",
       "       'Townes Van Zandt', 'Tracy Chapman', 'Tragically Hip', 'Train',\n",
       "       'Travis', 'Twenty One Pilots', 'U. D. O.', 'U-Kiss', 'U2', 'UB40',\n",
       "       'Ufo', 'Ugly Kid Joe', \"Ultramagnetic Mc's\", 'Ultravox',\n",
       "       'Uncle Kracker', 'Uncle Tupelo', 'Underoath', 'Underworld',\n",
       "       'Unearth', 'Ungu', 'Unkle', 'Unknown', 'Unseen', 'Unwritten Law',\n",
       "       'Uriah Heep', 'Used', 'Usher', 'Utada Hikaru', 'Utopia',\n",
       "       'Van Halen', 'Van Morrison', 'Vanessa Williams', 'Vangelis',\n",
       "       'Vanilla Ice', 'Velvet Underground', 'Vengaboys', 'Venom',\n",
       "       'Vera Lynn', 'Vertical Horizon', 'Veruca Salt', 'Verve',\n",
       "       'Vince Gill', 'Violent Femmes', 'Virgin Steele', 'Vonda Shepard',\n",
       "       'Vybz Kartel', 'Walk The Moon', 'Wanda Jackson', 'Wang Chung',\n",
       "       'Warren Zevon', 'W.A.S.P.', 'Waterboys', 'Waylon Jennings', 'Ween',\n",
       "       'Weezer', 'Weird Al Yankovic', 'Westlife', 'Wet Wet Wet', 'Wham!',\n",
       "       'Whiskeytown', 'The White Stripes', 'Whitesnake',\n",
       "       'Whitney Houston', 'Who', 'Widespread Panic', 'Will Smith',\n",
       "       'Willie Nelson', 'Wilson Phillips', 'Wilson Pickett',\n",
       "       'Wishbone Ash', 'Within Temptation', 'Wiz Khalifa', 'Wu-Tang Clan',\n",
       "       'Wyclef Jean', 'X', 'X Japan', 'X-Raided', 'X-Ray Spex', 'X-Treme',\n",
       "       'Xandria', 'Xavier Naidoo', 'Xavier Rudd', 'Xentrix', 'Xiu Xiu',\n",
       "       'Xscape', 'XTC', 'Xzibit', 'Yazoo', 'Yeah Yeah Yeahs', 'Yelawolf',\n",
       "       'Yello', 'Yellowcard', 'Yeng Constantino', 'Yes', 'YG',\n",
       "       'Ying Yang Twins', 'Yngwie Malmsteen', 'Yo Gotti', 'Yo La Tengo',\n",
       "       'Yoko Ono', 'Yolanda Adams', 'Yonder Mountain String Band',\n",
       "       'You Am I', 'Young Buck', 'Young Dro', 'Young Jeezy',\n",
       "       'Youngbloodz', 'Youth Of Today', 'Yukmouth', 'Yung Joc',\n",
       "       'Yusuf Islam', 'Z-Ro', 'Zac Brown Band', 'Zakk Wylde', 'Zao',\n",
       "       'Zayn Malik', 'Zebra', 'Zebrahead', 'Zed', 'Zero 7', 'Zeromancer',\n",
       "       'Ziggy Marley', 'Zoe', 'Zoegirl', 'Zornik', 'Zox', 'Zucchero',\n",
       "       'Zwan', 'ZZ Top', 'Joseph And The Amazing Technicolor Dreamcoat',\n",
       "       'Soundtracks', 'Van Der Graaf Generator', 'Various Artists',\n",
       "       'Zazie'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['artist'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neil Young           185\n",
       "Hank Williams Jr.    185\n",
       "Nazareth             184\n",
       "America              184\n",
       "Cliff Richard        184\n",
       "Indigo Girls         184\n",
       "Kiss                 183\n",
       "Johnny Cash          183\n",
       "Chris Rea            182\n",
       "Bon Jovi             181\n",
       "Dolly Parton         180\n",
       "Fleetwood Mac        180\n",
       "Rolling Stones       179\n",
       "Deep Purple          179\n",
       "Rod Stewart          178\n",
       "Roy Orbison          178\n",
       "The Beatles          178\n",
       "James Taylor         177\n",
       "Randy Travis         177\n",
       "Morrissey            177\n",
       "Name: artist, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['artist'].value_counts()[10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se construye la red neural recurrente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, ntoken, inputs_size, num_hidden_nodes, num_layers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, inputs_size)\n",
    "\n",
    "        self.rnn = nn.LSTM(inputs_size, num_hidden_nodes, num_layers, dropout=dropout)\n",
    "       \n",
    "        self.decoder = nn.Linear(num_hidden_nodes, ntoken)\n",
    "\n",
    "        if tie_weights:\n",
    "            if num_hidden_nodes != inputs_size:\n",
    "                raise ValueError('Cuando se usa la tied flag, num_hidden_nodes debe ser igual a emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.xavier_uniform_(self.encoder.weight)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        nn.init.xavier_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.num_layers, batch_size, self.num_hidden_nodes),\n",
    "                weight.new_zeros(self.num_layers, batch_size, self.num_hidden_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para construir el corpus, pre-procesar y tokenizar la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\diego\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dictionary = Dictionary() \n",
    "        lyrics = dataframe['text'].apply(self.pre_process)\n",
    "        train, test, y_train, y_test = train_test_split(lyrics, dataframe['artist'], test_size=0.3, random_state=1)\n",
    "        train, val, y_train, y_val = train_test_split(train, y_train, test_size=0.3, random_state=1)\n",
    "        self.train = self.tokenize(train.str.cat(sep=' end_song '))\n",
    "        self.valid = self.tokenize(val.str.cat(sep=' end_song '))\n",
    "        self.test = self.tokenize(test.str.cat(sep=' end_song '))\n",
    "        self.train_raw = train.str.cat(sep=' end_song ')\n",
    "        self.valid_raw = val.str.cat(sep=' end_song ')\n",
    "        self.test_raw = test.str.cat(sep=' end_song ')\n",
    "        \n",
    "    def pre_process(self,text):\n",
    "        text = text.replace(\"\\r\",\" \")\n",
    "        text = text.replace(\"\\n\",\" \")\n",
    "        text = text.replace(\"x2\",\" \")\n",
    "        text = text.replace(\"x3\",\" \")\n",
    "        text = text.replace(\"'\",\" \")\n",
    "        text = text.replace(\"end_song\",\" \")\n",
    "        text = text.lower()\n",
    "        table = str.maketrans('', '', '!\"#$%&\\()*+-/:;<=>?@[\\\\]^_`{|}~')\n",
    "        text = text.translate(table)\n",
    "        return(text)\n",
    "    \n",
    "    def tokenize(self, string):\n",
    "        tokens = 0\n",
    "        words = nltk.word_tokenize(string)\n",
    "        tokens += len(words)\n",
    "        for word in words:\n",
    "            self.dictionary.add_word(word)\n",
    "\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        words = nltk.word_tokenize(string)\n",
    "        for word in words:\n",
    "            ids[token] = self.dictionary.word2idx[word]\n",
    "            token += 1\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCorpus(subset):\n",
    "  return Corpus(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "  \n",
    "    nbatch = data.size(0) // batch_size\n",
    "    \n",
    "    # Corta elementos extra que no encajan limpiamente (recordatorios).\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Divide uniformemente los datos entre los batches.\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empaca los estados ocultos en nuevos tensores, para desacoplarlos de su historia.\n",
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "      return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "    \n",
    "def train(epoch, batch_size):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    cur_loss = 0\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "        # Reinicia el gradiente después de cada epoch (iteración).    \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        # El optimizador toma un paso y actualiza los pesos.\n",
    "#         optimizer.step()\n",
    "        \n",
    "        # `clip_grad_norm` ayuda previniendo el problema en RNN del gradiente explotando.helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip'])\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "#         print(str(args['log_interval']) + \", batch \" + str(batch))\n",
    "        if batch % args['log_interval'] == 0 and batch > 0:\n",
    "            cur_loss = total_loss / args['log_interval']\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:2.8f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.8f}'.format(\n",
    "                epoch, batch, len(train_data) // args['bptt'], lr,\n",
    "                elapsed * 1000 / args['log_interval'], cur_loss))\n",
    "            \n",
    "#                print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "#                     'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "#                 epoch, batch, len(train_data) // args['bptt'], lr,\n",
    "#                 elapsed * 1000 / args['log_interval'], cur_loss, 2**(cur_loss)))\n",
    "            \n",
    "            total_loss = 0\n",
    "\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un subconjunto de canciones con las letras de Bon Jovi y The Beatles, se tratará de construir una letra de canción inspirada en estos dos artistas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1198</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>A Shot Of Rhythm And Blues</td>\n",
       "      <td>/b/beatles/a+shot+of+rhythm+blues_20014867.html</td>\n",
       "      <td>Well, if your hands start a-clappin'  \\nAnd yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1199</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>Across The Universe</td>\n",
       "      <td>/b/beatles/across+the+universe_10026507.html</td>\n",
       "      <td>Words are flowing out like  \\nEndless rain int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>All I've Got To Do</td>\n",
       "      <td>/b/beatles/all+ive+got+to+do_10026646.html</td>\n",
       "      <td>Whenever I want you around, yeah  \\nAll I gott...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1201</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>And I Love Her</td>\n",
       "      <td>/b/beatles/and+i+love+her_10026463.html</td>\n",
       "      <td>I give her all my love  \\nThat's all I do  \\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1202</td>\n",
       "      <td>The Beatles</td>\n",
       "      <td>And Your Bird Can Sing</td>\n",
       "      <td>/b/beatles/and+your+bird+can+sing_10026364.html</td>\n",
       "      <td>You tell me that you've got everything you wan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           artist                        song  \\\n",
       "1198  The Beatles  A Shot Of Rhythm And Blues   \n",
       "1199  The Beatles         Across The Universe   \n",
       "1200  The Beatles          All I've Got To Do   \n",
       "1201  The Beatles              And I Love Her   \n",
       "1202  The Beatles      And Your Bird Can Sing   \n",
       "\n",
       "                                                 link  \\\n",
       "1198  /b/beatles/a+shot+of+rhythm+blues_20014867.html   \n",
       "1199     /b/beatles/across+the+universe_10026507.html   \n",
       "1200       /b/beatles/all+ive+got+to+do_10026646.html   \n",
       "1201          /b/beatles/and+i+love+her_10026463.html   \n",
       "1202  /b/beatles/and+your+bird+can+sing_10026364.html   \n",
       "\n",
       "                                                   text  \n",
       "1198  Well, if your hands start a-clappin'  \\nAnd yo...  \n",
       "1199  Words are flowing out like  \\nEndless rain int...  \n",
       "1200  Whenever I want you around, yeah  \\nAll I gott...  \n",
       "1201  I give her all my love  \\nThat's all I do  \\nA...  \n",
       "1202  You tell me that you've got everything you wan...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>2</td>\n",
       "      <td>355</td>\n",
       "      <td>359</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>Bon Jovi</td>\n",
       "      <td>Mrs. Robinson</td>\n",
       "      <td>/b/beatles/ill+get+you_10026295.html</td>\n",
       "      <td>I was dreaming of the past  \\nAnd my heart was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>181</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          artist           song                                  link  \\\n",
       "count        359            359                                   359   \n",
       "unique         2            355                                   359   \n",
       "top     Bon Jovi  Mrs. Robinson  /b/beatles/ill+get+you_10026295.html   \n",
       "freq         181              2                                     1   \n",
       "\n",
       "                                                     text  \n",
       "count                                                 359  \n",
       "unique                                                359  \n",
       "top     I was dreaming of the past  \\nAnd my heart was...  \n",
       "freq                                                    1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. se crea un subconjunto con los grupos que se usarán las letras\n",
    "subset = data[data['artist'].isin([\"Bon Jovi\", \"The Beatles\"])]\n",
    "display(subset.head())\n",
    "display(subset.describe())\n",
    "corpus = createCorpus(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. batchify\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, 5)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Construir modelo\n",
    "args = {\n",
    "    \n",
    "    \"lr\":10,\n",
    "    \"clip\":0.25,\n",
    "    \"epochs\":140, # limite superior de epoch\n",
    "    \"batch_size\":5,\n",
    "    \"bptt\":40,#backpropagation a través del tiempo(tamanio de secuencia)\n",
    "    \n",
    "    \"seed\":1,\n",
    "    \"log_interval\":50,\n",
    "    \"save\":\"model.pt\"\n",
    "}\n",
    "bptt = 40\n",
    "batch_size = 5\n",
    "ntokens = len(corpus.dictionary)\n",
    "# ntoken, inputs_size, num_hidden_nodes, num_layers, dropout=0.5, tie_weights=False\n",
    "model = RNNModel(ntokens, inputs_size = 300, num_hidden_nodes = 300, num_layers = 10, dropout = 0.2, tie_weights = True).to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    50/  223 batches | lr 20.00000000 | ms/batch 749.02 | loss 6.81731906\n",
      "| epoch   1 |   100/  223 batches | lr 20.00000000 | ms/batch 716.72 | loss 6.09458419\n",
      "| epoch   1 |   150/  223 batches | lr 20.00000000 | ms/batch 675.65 | loss 5.85832354\n",
      "| epoch   1 |   200/  223 batches | lr 20.00000000 | ms/batch 629.03 | loss 6.11647357\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch   1 | tiempo: 159.92329574s | perdida valida (loss) 6.15490952\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    50/  223 batches | lr 20.00000000 | ms/batch 641.93 | loss 5.98990718\n",
      "| epoch   2 |   100/  223 batches | lr 20.00000000 | ms/batch 641.27 | loss 5.88865850\n",
      "| epoch   2 |   150/  223 batches | lr 20.00000000 | ms/batch 677.51 | loss 5.72462654\n",
      "| epoch   2 |   200/  223 batches | lr 20.00000000 | ms/batch 644.93 | loss 6.03055925\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch   2 | tiempo: 152.70408726s | perdida valida (loss) 6.13768937\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    50/  223 batches | lr 20.00000000 | ms/batch 629.53 | loss 5.97658360\n",
      "| epoch   3 |   100/  223 batches | lr 20.00000000 | ms/batch 648.60 | loss 5.86966249\n",
      "| epoch   3 |   150/  223 batches | lr 20.00000000 | ms/batch 625.93 | loss 5.68640109\n",
      "| epoch   3 |   200/  223 batches | lr 20.00000000 | ms/batch 615.22 | loss 5.99328074\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch   3 | tiempo: 148.20637131s | perdida valida (loss) 6.15777898\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    50/  223 batches | lr 10.00000000 | ms/batch 676.27 | loss 5.90335810\n",
      "| epoch   4 |   100/  223 batches | lr 10.00000000 | ms/batch 617.20 | loss 5.78087036\n",
      "| epoch   4 |   150/  223 batches | lr 10.00000000 | ms/batch 612.28 | loss 5.59268378\n",
      "| epoch   4 |   200/  223 batches | lr 10.00000000 | ms/batch 613.62 | loss 5.87486172\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch   4 | tiempo: 146.34329772s | perdida valida (loss) 6.08232175\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    50/  223 batches | lr 10.00000000 | ms/batch 653.33 | loss 5.84694186\n",
      "| epoch   5 |   100/  223 batches | lr 10.00000000 | ms/batch 629.88 | loss 5.74082788\n",
      "| epoch   5 |   150/  223 batches | lr 10.00000000 | ms/batch 617.16 | loss 5.55527811\n",
      "| epoch   5 |   200/  223 batches | lr 10.00000000 | ms/batch 631.20 | loss 5.84205653\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch   5 | tiempo: 147.40419078s | perdida valida (loss) 6.09265647\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    50/  223 batches | lr 5.00000000 | ms/batch 650.15 | loss 5.71075532\n",
      "| epoch   6 |   100/  223 batches | lr 5.00000000 | ms/batch 610.08 | loss 5.63454822\n",
      "| epoch   6 |   150/  223 batches | lr 5.00000000 | ms/batch 661.30 | loss 5.46025229\n",
      "| epoch   6 |   200/  223 batches | lr 5.00000000 | ms/batch 663.22 | loss 5.74283970\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch   6 | tiempo: 150.48112941s | perdida valida (loss) 6.28382880\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    50/  223 batches | lr 2.50000000 | ms/batch 603.40 | loss 5.66848711\n",
      "| epoch   7 |   100/  223 batches | lr 2.50000000 | ms/batch 628.40 | loss 5.61637882\n",
      "| epoch   7 |   150/  223 batches | lr 2.50000000 | ms/batch 621.18 | loss 5.43603987\n",
      "| epoch   7 |   200/  223 batches | lr 2.50000000 | ms/batch 640.16 | loss 5.73775555\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch   7 | tiempo: 145.99288106s | perdida valida (loss) 6.21170395\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    50/  223 batches | lr 1.25000000 | ms/batch 586.42 | loss 5.72549573\n",
      "| epoch   8 |   100/  223 batches | lr 1.25000000 | ms/batch 608.93 | loss 5.63545096\n",
      "| epoch   8 |   150/  223 batches | lr 1.25000000 | ms/batch 598.06 | loss 5.47080024\n",
      "| epoch   8 |   200/  223 batches | lr 1.25000000 | ms/batch 604.25 | loss 5.77224349\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch   8 | tiempo: 139.72627974s | perdida valida (loss) 6.22399528\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    50/  223 batches | lr 0.62500000 | ms/batch 591.98 | loss 5.78749284\n",
      "| epoch   9 |   100/  223 batches | lr 0.62500000 | ms/batch 589.00 | loss 5.64563527\n",
      "| epoch   9 |   150/  223 batches | lr 0.62500000 | ms/batch 581.15 | loss 5.51647028\n",
      "| epoch   9 |   200/  223 batches | lr 0.62500000 | ms/batch 595.70 | loss 5.80836103\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch   9 | tiempo: 137.97963929s | perdida valida (loss) 6.25020195\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    50/  223 batches | lr 0.31250000 | ms/batch 616.10 | loss 5.81117711\n",
      "| epoch  10 |   100/  223 batches | lr 0.31250000 | ms/batch 599.45 | loss 5.68329583\n",
      "| epoch  10 |   150/  223 batches | lr 0.31250000 | ms/batch 591.67 | loss 5.55390811\n",
      "| epoch  10 |   200/  223 batches | lr 0.31250000 | ms/batch 597.46 | loss 5.85971396\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  10 | tiempo: 140.24731994s | perdida valida (loss) 6.14116734\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |    50/  223 batches | lr 0.15625000 | ms/batch 601.10 | loss 5.83688245\n",
      "| epoch  11 |   100/  223 batches | lr 0.15625000 | ms/batch 582.73 | loss 5.80030821\n",
      "| epoch  11 |   150/  223 batches | lr 0.15625000 | ms/batch 582.13 | loss 5.58786928\n",
      "| epoch  11 |   200/  223 batches | lr 0.15625000 | ms/batch 590.46 | loss 5.90621456\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  11 | tiempo: 137.65476799s | perdida valida (loss) 6.14242789\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    50/  223 batches | lr 0.07812500 | ms/batch 602.99 | loss 5.86128402\n",
      "| epoch  12 |   100/  223 batches | lr 0.07812500 | ms/batch 576.87 | loss 5.85274393\n",
      "| epoch  12 |   150/  223 batches | lr 0.07812500 | ms/batch 584.52 | loss 5.64211314\n",
      "| epoch  12 |   200/  223 batches | lr 0.07812500 | ms/batch 576.75 | loss 5.94013781\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  12 | tiempo: 136.84762883s | perdida valida (loss) 6.10191679\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |    50/  223 batches | lr 0.03906250 | ms/batch 592.88 | loss 5.89233020\n",
      "| epoch  13 |   100/  223 batches | lr 0.03906250 | ms/batch 576.41 | loss 5.87535725\n",
      "| epoch  13 |   150/  223 batches | lr 0.03906250 | ms/batch 584.69 | loss 5.64500530\n",
      "| epoch  13 |   200/  223 batches | lr 0.03906250 | ms/batch 588.83 | loss 6.06851454\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  13 | tiempo: 136.90271616s | perdida valida (loss) 6.07874120\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |    50/  223 batches | lr 0.03906250 | ms/batch 625.06 | loss 5.87396604\n",
      "| epoch  14 |   100/  223 batches | lr 0.03906250 | ms/batch 602.58 | loss 5.83611699\n",
      "| epoch  14 |   150/  223 batches | lr 0.03906250 | ms/batch 625.90 | loss 5.63907617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  14 |   200/  223 batches | lr 0.03906250 | ms/batch 626.80 | loss 5.97232628\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  14 | tiempo: 144.99377966s | perdida valida (loss) 6.09501762\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |    50/  223 batches | lr 0.01953125 | ms/batch 657.87 | loss 6.02371881\n",
      "| epoch  15 |   100/  223 batches | lr 0.01953125 | ms/batch 642.91 | loss 5.88924499\n",
      "| epoch  15 |   150/  223 batches | lr 0.01953125 | ms/batch 636.97 | loss 5.62849525\n",
      "| epoch  15 |   200/  223 batches | lr 0.01953125 | ms/batch 627.13 | loss 6.03856751\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  15 | tiempo: 148.86008620s | perdida valida (loss) 6.07288787\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |    50/  223 batches | lr 0.01953125 | ms/batch 613.26 | loss 5.95262178\n",
      "| epoch  16 |   100/  223 batches | lr 0.01953125 | ms/batch 593.88 | loss 5.87127005\n",
      "| epoch  16 |   150/  223 batches | lr 0.01953125 | ms/batch 599.07 | loss 5.62893805\n",
      "| epoch  16 |   200/  223 batches | lr 0.01953125 | ms/batch 589.76 | loss 6.02254064\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  16 | tiempo: 139.47966242s | perdida valida (loss) 6.05296194\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |    50/  223 batches | lr 0.01953125 | ms/batch 590.44 | loss 5.95537070\n",
      "| epoch  17 |   100/  223 batches | lr 0.01953125 | ms/batch 583.74 | loss 5.88525517\n",
      "| epoch  17 |   150/  223 batches | lr 0.01953125 | ms/batch 567.60 | loss 5.63147701\n",
      "| epoch  17 |   200/  223 batches | lr 0.01953125 | ms/batch 584.44 | loss 5.99544004\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  17 | tiempo: 135.76010227s | perdida valida (loss) 6.06744826\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |    50/  223 batches | lr 0.00976562 | ms/batch 596.29 | loss 5.99847056\n",
      "| epoch  18 |   100/  223 batches | lr 0.00976562 | ms/batch 587.65 | loss 5.91132298\n",
      "| epoch  18 |   150/  223 batches | lr 0.00976562 | ms/batch 630.91 | loss 5.63024546\n",
      "| epoch  18 |   200/  223 batches | lr 0.00976562 | ms/batch 649.86 | loss 6.04731853\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  18 | tiempo: 145.22602820s | perdida valida (loss) 6.04512420\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |    50/  223 batches | lr 0.00976562 | ms/batch 654.87 | loss 5.97281322\n",
      "| epoch  19 |   100/  223 batches | lr 0.00976562 | ms/batch 644.80 | loss 5.88967064\n",
      "| epoch  19 |   150/  223 batches | lr 0.00976562 | ms/batch 645.87 | loss 5.63226951\n",
      "| epoch  19 |   200/  223 batches | lr 0.00976562 | ms/batch 639.82 | loss 6.04833813\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  19 | tiempo: 151.11819720s | perdida valida (loss) 6.03989066\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |    50/  223 batches | lr 0.00976562 | ms/batch 650.29 | loss 5.95646923\n",
      "| epoch  20 |   100/  223 batches | lr 0.00976562 | ms/batch 659.52 | loss 5.89073820\n",
      "| epoch  20 |   150/  223 batches | lr 0.00976562 | ms/batch 645.17 | loss 5.62732798\n",
      "| epoch  20 |   200/  223 batches | lr 0.00976562 | ms/batch 642.50 | loss 6.04213152\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  20 | tiempo: 152.03489900s | perdida valida (loss) 6.04198129\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |    50/  223 batches | lr 0.00488281 | ms/batch 655.48 | loss 6.00096556\n",
      "| epoch  21 |   100/  223 batches | lr 0.00488281 | ms/batch 650.41 | loss 5.91067562\n",
      "| epoch  21 |   150/  223 batches | lr 0.00488281 | ms/batch 635.57 | loss 5.63571157\n",
      "| epoch  21 |   200/  223 batches | lr 0.00488281 | ms/batch 655.66 | loss 6.06687019\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  21 | tiempo: 151.34596419s | perdida valida (loss) 6.02854982\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |    50/  223 batches | lr 0.00488281 | ms/batch 666.82 | loss 5.96768795\n",
      "| epoch  22 |   100/  223 batches | lr 0.00488281 | ms/batch 638.75 | loss 5.89988906\n",
      "| epoch  22 |   150/  223 batches | lr 0.00488281 | ms/batch 648.40 | loss 5.64053591\n",
      "| epoch  22 |   200/  223 batches | lr 0.00488281 | ms/batch 646.09 | loss 6.08172812\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  22 | tiempo: 151.66565275s | perdida valida (loss) 6.02546097\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |    50/  223 batches | lr 0.00488281 | ms/batch 659.56 | loss 5.95381058\n",
      "| epoch  23 |   100/  223 batches | lr 0.00488281 | ms/batch 643.34 | loss 5.89588175\n",
      "| epoch  23 |   150/  223 batches | lr 0.00488281 | ms/batch 645.92 | loss 5.64031625\n",
      "| epoch  23 |   200/  223 batches | lr 0.00488281 | ms/batch 636.04 | loss 6.07511057\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  23 | tiempo: 151.14649940s | perdida valida (loss) 6.02445374\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |    50/  223 batches | lr 0.00488281 | ms/batch 648.21 | loss 5.94507857\n",
      "| epoch  24 |   100/  223 batches | lr 0.00488281 | ms/batch 644.57 | loss 5.89318879\n",
      "| epoch  24 |   150/  223 batches | lr 0.00488281 | ms/batch 632.18 | loss 5.63934137\n",
      "| epoch  24 |   200/  223 batches | lr 0.00488281 | ms/batch 645.54 | loss 6.06883662\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  24 | tiempo: 150.07568908s | perdida valida (loss) 6.02497170\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |    50/  223 batches | lr 0.00244141 | ms/batch 650.74 | loss 5.96063367\n",
      "| epoch  25 |   100/  223 batches | lr 0.00244141 | ms/batch 641.41 | loss 5.91014039\n",
      "| epoch  25 |   150/  223 batches | lr 0.00244141 | ms/batch 636.34 | loss 5.64536465\n",
      "| epoch  25 |   200/  223 batches | lr 0.00244141 | ms/batch 651.06 | loss 6.07754276\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  25 | tiempo: 150.47039676s | perdida valida (loss) 6.01560757\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |    50/  223 batches | lr 0.00244141 | ms/batch 660.14 | loss 5.94821997\n",
      "| epoch  26 |   100/  223 batches | lr 0.00244141 | ms/batch 638.91 | loss 5.89967526\n",
      "| epoch  26 |   150/  223 batches | lr 0.00244141 | ms/batch 640.22 | loss 5.65011345\n",
      "| epoch  26 |   200/  223 batches | lr 0.00244141 | ms/batch 642.19 | loss 6.08263864\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  26 | tiempo: 151.27482700s | perdida valida (loss) 6.01623371\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |    50/  223 batches | lr 0.00122070 | ms/batch 652.06 | loss 5.95594394\n",
      "| epoch  27 |   100/  223 batches | lr 0.00122070 | ms/batch 642.74 | loss 5.90548961\n",
      "| epoch  27 |   150/  223 batches | lr 0.00122070 | ms/batch 633.49 | loss 5.65581236\n",
      "| epoch  27 |   200/  223 batches | lr 0.00122070 | ms/batch 641.89 | loss 6.07352314\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  27 | tiempo: 150.04865122s | perdida valida (loss) 6.01120180\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  28 |    50/  223 batches | lr 0.00122070 | ms/batch 648.60 | loss 5.95031078\n",
      "| epoch  28 |   100/  223 batches | lr 0.00122070 | ms/batch 657.92 | loss 5.90260273\n",
      "| epoch  28 |   150/  223 batches | lr 0.00122070 | ms/batch 638.08 | loss 5.65281574\n",
      "| epoch  28 |   200/  223 batches | lr 0.00122070 | ms/batch 641.84 | loss 6.07826766\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  28 | tiempo: 150.87532568s | perdida valida (loss) 6.00963741\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |    50/  223 batches | lr 0.00122070 | ms/batch 663.18 | loss 5.94806618\n",
      "| epoch  29 |   100/  223 batches | lr 0.00122070 | ms/batch 639.81 | loss 5.89983987\n",
      "| epoch  29 |   150/  223 batches | lr 0.00122070 | ms/batch 644.90 | loss 5.65147562\n",
      "| epoch  29 |   200/  223 batches | lr 0.00122070 | ms/batch 637.14 | loss 6.07990170\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  29 | tiempo: 150.85787177s | perdida valida (loss) 6.00963712\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |    50/  223 batches | lr 0.00122070 | ms/batch 663.30 | loss 5.94213820\n",
      "| epoch  30 |   100/  223 batches | lr 0.00122070 | ms/batch 627.77 | loss 5.89559456\n",
      "| epoch  30 |   150/  223 batches | lr 0.00122070 | ms/batch 599.54 | loss 5.65116775\n",
      "| epoch  30 |   200/  223 batches | lr 0.00122070 | ms/batch 603.34 | loss 6.08713919\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  30 | tiempo: 144.68837810s | perdida valida (loss) 6.00967516\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |    50/  223 batches | lr 0.00061035 | ms/batch 609.42 | loss 5.94861784\n",
      "| epoch  31 |   100/  223 batches | lr 0.00061035 | ms/batch 592.65 | loss 5.90116034\n",
      "| epoch  31 |   150/  223 batches | lr 0.00061035 | ms/batch 582.69 | loss 5.65992405\n",
      "| epoch  31 |   200/  223 batches | lr 0.00061035 | ms/batch 582.89 | loss 6.07438099\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  31 | tiempo: 138.41106176s | perdida valida (loss) 6.01001295\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |    50/  223 batches | lr 0.00030518 | ms/batch 590.20 | loss 5.94635870\n",
      "| epoch  32 |   100/  223 batches | lr 0.00030518 | ms/batch 583.75 | loss 5.90016204\n",
      "| epoch  32 |   150/  223 batches | lr 0.00030518 | ms/batch 577.70 | loss 5.65931347\n",
      "| epoch  32 |   200/  223 batches | lr 0.00030518 | ms/batch 589.83 | loss 6.07194049\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  32 | tiempo: 136.95946527s | perdida valida (loss) 6.01034411\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |    50/  223 batches | lr 0.00015259 | ms/batch 619.54 | loss 5.94939999\n",
      "| epoch  33 |   100/  223 batches | lr 0.00015259 | ms/batch 658.19 | loss 5.89856493\n",
      "| epoch  33 |   150/  223 batches | lr 0.00015259 | ms/batch 590.07 | loss 5.66367491\n",
      "| epoch  33 |   200/  223 batches | lr 0.00015259 | ms/batch 596.64 | loss 6.07267865\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  33 | tiempo: 142.57553458s | perdida valida (loss) 6.01045181\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |    50/  223 batches | lr 0.00007629 | ms/batch 584.93 | loss 5.94883522\n",
      "| epoch  34 |   100/  223 batches | lr 0.00007629 | ms/batch 591.39 | loss 5.90143127\n",
      "| epoch  34 |   150/  223 batches | lr 0.00007629 | ms/batch 584.20 | loss 5.66008640\n",
      "| epoch  34 |   200/  223 batches | lr 0.00007629 | ms/batch 586.92 | loss 6.07199139\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  34 | tiempo: 137.25750351s | perdida valida (loss) 6.01055323\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |    50/  223 batches | lr 0.00003815 | ms/batch 584.61 | loss 5.94284606\n",
      "| epoch  35 |   100/  223 batches | lr 0.00003815 | ms/batch 556.65 | loss 5.90059314\n",
      "| epoch  35 |   150/  223 batches | lr 0.00003815 | ms/batch 563.40 | loss 5.66187475\n",
      "| epoch  35 |   200/  223 batches | lr 0.00003815 | ms/batch 567.09 | loss 6.07570278\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  35 | tiempo: 133.03162169s | perdida valida (loss) 6.01057816\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |    50/  223 batches | lr 0.00001907 | ms/batch 603.54 | loss 5.94852252\n",
      "| epoch  36 |   100/  223 batches | lr 0.00001907 | ms/batch 610.76 | loss 5.90194928\n",
      "| epoch  36 |   150/  223 batches | lr 0.00001907 | ms/batch 656.68 | loss 5.66608562\n",
      "| epoch  36 |   200/  223 batches | lr 0.00001907 | ms/batch 647.56 | loss 6.07485003\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  36 | tiempo: 148.14341736s | perdida valida (loss) 6.01059750\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |    50/  223 batches | lr 0.00000954 | ms/batch 665.93 | loss 5.94582727\n",
      "| epoch  37 |   100/  223 batches | lr 0.00000954 | ms/batch 648.01 | loss 5.89924795\n",
      "| epoch  37 |   150/  223 batches | lr 0.00000954 | ms/batch 648.46 | loss 5.66388131\n",
      "| epoch  37 |   200/  223 batches | lr 0.00000954 | ms/batch 646.25 | loss 6.06910159\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  37 | tiempo: 152.41755819s | perdida valida (loss) 6.01061166\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |    50/  223 batches | lr 0.00000477 | ms/batch 655.61 | loss 5.94521180\n",
      "| epoch  38 |   100/  223 batches | lr 0.00000477 | ms/batch 649.29 | loss 5.90114361\n",
      "| epoch  38 |   150/  223 batches | lr 0.00000477 | ms/batch 646.55 | loss 5.66274529\n",
      "| epoch  38 |   200/  223 batches | lr 0.00000477 | ms/batch 727.68 | loss 6.06699617\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  38 | tiempo: 156.91010737s | perdida valida (loss) 6.01061597\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |    50/  223 batches | lr 0.00000238 | ms/batch 661.58 | loss 5.94515940\n",
      "| epoch  39 |   100/  223 batches | lr 0.00000238 | ms/batch 647.81 | loss 5.90067924\n",
      "| epoch  39 |   150/  223 batches | lr 0.00000238 | ms/batch 639.51 | loss 5.67044892\n",
      "| epoch  39 |   200/  223 batches | lr 0.00000238 | ms/batch 646.71 | loss 6.07091628\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  39 | tiempo: 151.59617639s | perdida valida (loss) 6.01061838\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |    50/  223 batches | lr 0.00000119 | ms/batch 666.76 | loss 5.94446837\n",
      "| epoch  40 |   100/  223 batches | lr 0.00000119 | ms/batch 636.49 | loss 5.90193254\n",
      "| epoch  40 |   150/  223 batches | lr 0.00000119 | ms/batch 650.97 | loss 5.66687834\n",
      "| epoch  40 |   200/  223 batches | lr 0.00000119 | ms/batch 635.32 | loss 6.07001244\n",
      "-----------------------------------------------------------------------------------------\n",
      "| fin del epoch  40 | tiempo: 151.52888799s | perdida valida (loss) 6.01061958\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| Fin del entrenamiento | perdida en pruebas (loss) 5.98033109 | ppl de pruebas    63.13\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 4. entrenar modelo\n",
    "# Ciclo entre epochs.\n",
    "lr = 20\n",
    "best_val_loss = None\n",
    "epochs = 40\n",
    "\n",
    "# Inicializando el optimizador\n",
    "#learning_rate = args['lr']\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "try:\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(epoch, batch_size)\n",
    "        \n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| fin del epoch {:3d} | tiempo: {:5.8f}s | perdida valida (loss) {:5.8f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss))\n",
    "        print('-' * 89)\n",
    "      \n",
    "        # Guardando el modelo si la perdida de validación (validation loss).\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(\"model.pt\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Recorte la tasa de aprendizaje si no se han observado mejoras en el conjunto de datos de validación.\n",
    "            lr = lr / 2.0\n",
    "        #if lr < 0.5:\n",
    "            #lr=0.5\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Saliendo temprano del entrenamiento')\n",
    "    logging.debug('-' * 89)\n",
    "    logging.debug('Saliendo del entrenamiento temprano')\n",
    "    \n",
    "\n",
    "# Carga el mejor modelo.\n",
    "with open(\"model.pt\", 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # después de cargar los parámetros de la Red Neural Recurrente se aplica flatten_parameters\n",
    "    # que hace que los parámetros estén contínuos y esto repercute agilizando el procesamiento\n",
    "    model.rnn.flatten_parameters()\n",
    "\n",
    "# Se corren los datos de prueba\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| Fin del entrenamiento | perdida en pruebas (loss) {:5.8f} | ppl de pruebas {:8.2f}'.format(\n",
    "    test_loss, 2**(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado se genera un archivo de salida con una canción de 200 palabras, primero se generó una salida en el archivo BonBeatles.txt y luego en BonBeatles2.txt, estos archivos son generados con base en el modelo entrenado como se ve en el código a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Generando 0/200 palabras\n",
      "| Generando 30/200 palabras\n",
      "| Generando 60/200 palabras\n",
      "| Generando 90/200 palabras\n",
      "| Generando 120/200 palabras\n",
      "| Generando 150/200 palabras\n",
      "| Generando 180/200 palabras\n"
     ]
    }
   ],
   "source": [
    "# 5. generar salida\n",
    "generate_args={\n",
    "    \"temperature\": 1, # temperatura - a un valor más alto se incrementa la diversidad\n",
    "    \"words\":200, #numero de palabras a generar\n",
    "    \"outf\":\"BonBeatles2.txt\",\n",
    "    \"log_interval\":30,\n",
    "}\n",
    "\n",
    "with open(\"model.pt\", 'rb') as f:\n",
    "    model = torch.load(f).to(device)\n",
    "model.eval()\n",
    "seed_word = \"hard\"\n",
    "seed=torch.LongTensor(1,1).to(device)\n",
    "seed[0]=corpus.dictionary.word2idx[seed_word]\n",
    "hidden = model.init_hidden(1)\n",
    "#input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "input = seed\n",
    "with open(generate_args['outf'], 'w') as outf:\n",
    "    outf.write(seed_word + ' ')\n",
    "    with torch.no_grad():  # sin rastreo del historial\n",
    "        for i in range(generate_args['words']):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().div(generate_args['temperature']).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "            \n",
    "            outf.write(word + ' ')\n",
    "\n",
    "            if i % generate_args['log_interval'] == 0:\n",
    "                print('| Generando {}/{} palabras'.format(i, generate_args['words']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se muestra a continuación los resultados de las dos canciones generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard be it gothic that let eyes d on me sugar lie like ledge so times hole party you you you easier just shattered i am go that have just what of she the thought m for it i but ve the know love my worked to boy deep my better close just like when you in be of last little ring won , , i love night running no paychecks in m to , there along that said living over like brand mckenzie ll take , love ve down miss candidates wear on but wasting never don get you for some ask . now was hands , , sleep you got in you na being a moscow but baby alone is want accused her k two ve man in a , disappear want don real homes that my in always just love mind , else m even me magic story you i for soul pay want me didn bend ve s on my to , no anticipation what bleedin gim night , bang , you for it til m so you you to who someday mystery the on the around detroit let you ll hand don blues and youre her \n"
     ]
    }
   ],
   "source": [
    "f = open(\"BonBeatles.txt\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard you hug more world t don nah ain a help family schon own go oneway stations my now die maybe no drew m me didididi a still sleep it free up what and some t s just way yeah in re talk i ll can man up it be alive thought hurts and of touch its roads re norwegian wings for , hears both go a i dreamer , man some sins i me , you i i re might you a good just the but weren come and we tear meet . i ordinary bad , about take time and a get wall a and difference know have all know ll got . you future she without out t and when comes t she street her rest t float i you the or bring life knows our ever om out i she s of to turns could secrets time who suntan heart day i than re this bring is drink na baby captain ... i my dance mm lips na in the , leave cold will love man talk in i , it now life buy nobody s off night streets you drive my s twolane , me day \n"
     ]
    }
   ],
   "source": [
    "f = open(\"BonBeatles2.txt\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusiones\n",
    "* Los resultados demuestran que mientras más entrenamiento se le da al modelo se mejoran los resultados, algo importante a destacar es que se pueden tener resultados bastante interesantes, en este caso se tiene la limitante de los recursos para procesamiento, ya que el equipo no es para uso exclusivo, sino que también es utilizado para otras labores.\n",
    "* Así como menciona el paper al que se hace referencia, el resultado que se puede llegar a tener es bastante bueno y abierto a aplicaciones sumamente interesantes, que pueden llegar a remoldear muchas de las cosas que se hacen actualmente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
